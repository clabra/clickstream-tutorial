# Define una fuente de tipo Avro
collector1.sources=r1
collector1.sources.r1.type=avro
collector1.sources.r1.bind=0.0.0.0
collector1.sources.r1.port=4141
collector1.sources.r1.channels=ch1
# Como canal se usaran ficheros
# Se define el directorio donde se van a hacer los checkpoints y 
# dos directorios donde se dejaran los ficheros de logs
# Habitualmente se usan multiles directorios en discos separados para mejorar el rendimiento del canal 
collector1.channels=ch1
collector1.channels.ch1.type=FILE
collector1.channels.ch1.checkpointDir=/opt/flume/ch1/checkpoint
collector1.channels.ch1.dataDirs=/opt/flume/ch1/data1,/opt/flume/ch1/data2
# Define sumideros de tipo HDFS para guardar los eventos como texto
# Se usan dos sumideros para distribuir la carga
collector1.sinks=k1 k2
collector1.sinks.k1.type=hdfs
collector1.sinks.k1.channel=ch1
# Los ficheros se particionan por fecha
#collector1.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector1.sinks.k1.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
# Se añade este prefijo a los ficheros que se creen
collector1.sinks.k1.hdfs.filePrefix=combined
# Se añade este sufijo a los ficheros que se creen
collector1.sinks.k1.hdfs.fileSuffix=.log
collector1.sinks.k1.hdfs.fileType=DataStream
collector1.sinks.k1.hdfs.writeFormat=Text
# Los ficheros HDFS se van rotando (se cierra el actual y se crea uno nuevo) periodicamente usando diversos criterios (tiempo trasncurrido, tamaño de los datos o numero de eventos). 
# El path HDFS puede contener secuencias formateadas que seran reemplazadas para generar el nombre 
# Número de eventos escritos al fichero del canal antes de volcar a HDFS
collector1.sinks.k1.hdfs.batchSize=1000 
# Rota los ficheros cada 10000 eventos o 30 segundos
collector1.sinks.k1.hdfs.rollInterval=30
collector1.sinks.k1.hdfs.rollCount=10000
# No se rota por tamaño
collector1.sinks.k1.hdfs.rollSize=0
collector1.sinks.k2.type=hdfs
collector1.sinks.k2.channel=ch1
# Particionado de los datos por fecha
#collector1.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=%Y/month=%m/day=%d
collector1.sinks.k2.hdfs.path=/etl/BI/casualcyclist/clicks/rawlogs/year=2014/month=10/day=10
collector1.sinks.k2.hdfs.filePrefix=combined
collector1.sinks.k2.hdfs.fileSuffix=.log
# Puede ser DataStream, SequenceFile (pares clave-valor binarios) o CompressedStream (comprimido)
collector1.sinks.k2.hdfs.fileType=DataStream
collector1.sinks.k2.hdfs.writeFormat=Text
collector1.sinks.k2.hdfs.batchSize=1000 
collector1.sinks.k2.hdfs.rollInterval=30
collector1.sinks.k2.hdfs.rollCount=10000
collector1.sinks.k2.hdfs.rollSize=0
# Definimos un grupo de dos sumidores con balanceo de carga de tipo round robin
collector1.sinkgroups=g1
collector1.sinkgroups.g1.sinks=k1 k2
collector1.sinkgroups.g1.processor.type=load_balance
collector1.sinkgroups.g1.processor.selector=round_robin
collector1.sinkgroups.g1.processor.backoff=true

